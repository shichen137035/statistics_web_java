{
  "meta.title": "Ch.4 — Variance Comparison · Cramér–Rao · Fisher Information",
  "intro.title": "Variance, Cramér–Rao Bound, and Fisher Information",
  "intro.lead": "We compare variances among unbiased estimators, establish the Cramér–Rao lower bound under regularity, and formalize Fisher information—the quantity that governs the sharpness of unbiased estimation.",

  "sec.varcmp.title": "Variance Comparison (Unbiased Estimators)",
  "sec.varcmp.prop": "Among unbiased estimators of the same target $q(\\theta)$, the one with uniformly smaller variance is preferable under squared–error risk. Rao–Blackwell provides a constructive way to reduce variance by conditioning on a sufficient statistic.",
  "sec.varcmp.eq": "$$\\text{If }\\mathbb E_\\theta[T_1]=\\mathbb E_\\theta[T_2]=q(\\theta)\\;\\forall\\theta\\text{ and }\\operatorname{Var}_\\theta(T_1)\\le\\operatorname{Var}_\\theta(T_2)\\;\\forall\\theta,\\text{ then }T_1\\text{ dominates }T_2.$$",
  "sec.varcmp.ex1.title": "Example: Estimating $p$ in Bernoulli($p$)",
  "sec.varcmp.ex1.text": "Let $\\hat p_1=X_1$ and $\\hat p_2=\\bar X$. Both are unbiased for $p$, but $\\operatorname{Var}(X_1)=p(1-p)\\ge p(1-p)/n=\\operatorname{Var}(\\bar X)$. Hence $\\bar X$ dominates $X_1$ in variance.",
  "sec.varcmp.ex1.code": "set.seed(1)\ncompare_var <- function(n=20, p=0.4, B=20000){\n  s1 <- s2 <- numeric(B)\n  for(b in 1:B){ x <- rbinom(n, 1, p); s1[b] <- x[1]; s2[b] <- mean(x) }\n  c(var_X1 = var(s1), var_Xbar = var(s2))\n}\nprint(compare_var())",
  "sec.varcmp.preview": "This principle motivates lower bounds: if no unbiased estimator can beat a variance floor, an estimator that attains it is optimal in this class.",

  "sec.crlb.title": "Cramér–Rao Lower Bound (CRLB)",
  "sec.crlb.thm": "Under regularity and for an unbiased estimator $T$ of $q(\\theta)$, $$\\operatorname{Var}_\\theta(T)\\;\\ge\\;\\frac{\\big(q'(\\theta)\\big)^2}{I_n(\\theta)},$$ where $I_n(\\theta)$ is the Fisher information in $n$ observations.",
  "sec.crlb.eq": "$$I_n(\\theta)=n\\,I_1(\\theta),\\qquad I_1(\\theta)=\\operatorname{Var}_\\theta\\!\\left[\\partial_\\theta\\log f(X\\mid\\theta)\\right].$$",
  "sec.crlb.proof.label": "Proof sketch:",
  "sec.crlb.proof.s1": "Differentiate $\\mathbb E_\\theta[T]=q(\\theta)$ under the integral; obtain $q'(\\theta)=\\operatorname{Cov}_\\theta\\big(T,\\,\\partial_\\theta\\log f(X\\mid\\theta)\\big)$.",
  "sec.crlb.proof.s2": "Apply Cauchy–Schwarz to the covariance: $q'(\\theta)^2\\le\\operatorname{Var}(T)\\,I_n(\\theta)$.",
  "sec.crlb.proof.s3": "Rearrange to get the claimed lower bound.",
  "sec.crlb.ex1.title": "Example: Normal mean with known variance",
  "sec.crlb.ex1.text": "For $X_i\\sim\\mathcal N(\\mu,\\sigma^2)$ with known $\\sigma^2$, $I_n(\\mu)=n/\\sigma^2$. The sample mean attains CRLB since $\\operatorname{Var}(\\bar X)=\\sigma^2/n$.",
  "sec.crlb.ex1.code": "set.seed(2)\nn <- 100; sigma <- 2; mu <- 0; B <- 10000\nxb <- replicate(B, mean(rnorm(n, mu, sigma)))\ncat(\"empirical Var(Xbar)=\", var(xb), \" CRLB=\", sigma^2/n, \"\\n\")",
  "sec.crlb.ex2.title": "Example: Exponential rate $\\lambda$ (unbiased estimator vs CRLB)",
  "sec.crlb.ex2.text1": "If $X_i\\sim\\mathrm{Exp}(\\lambda)$ (rate), then $I_n(\\lambda)=n/\\lambda^2$. The estimator $\\tilde\\lambda=(n-1)/\\sum_i X_i$ is unbiased for $\\lambda$ when $n>1$.",
  "sec.crlb.ex2.eq": "$$\\operatorname{Var}(\\tilde\\lambda)=\\frac{\\lambda^2}{n-2}\\quad(n>2),\\qquad \\text{CRLB}=\\frac{\\lambda^2}{n}.$$",
  "sec.crlb.ex2.text2": "Hence $\\tilde\\lambda$ does not attain CRLB for finite $n$ (variance is larger), illustrating that the bound may be unattainable in small samples.",
  "sec.crlb.ex2.code": "set.seed(3)\nsim_exp <- function(n=6, lambda=2, B=20000){\n  est <- replicate(B, {x <- rexp(n, rate=lambda); (n-1)/sum(x)})\n  c(empVar = var(est), crlb = lambda^2/n)\n}\nprint(sim_exp())",

  "sec.fisher.title": "Fisher Information",
  "sec.fisher.def": "For a regular parametric model with density $f(x\\mid\\theta)$, Fisher information can be written as the variance of the score or the negative expected Hessian of the log-likelihood.",
  "sec.fisher.eq1": "$$I_1(\\theta)=\\operatorname{Var}_\\theta\\!\\left[\\partial_\\theta\\log f(X\\mid\\theta)\\right],\\qquad I_n(\\theta)=n\\,I_1(\\theta).$$",
  "sec.fisher.eq2": "$$I_1(\\theta)=-\\,\\mathbb E_\\theta\\big[\\partial_\\theta^2\\log f(X\\mid\\theta)\\big].$$",
  "sec.fisher.props.title": "Basic Properties",
  "sec.fisher.p1": "Additivity for i.i.d. samples: $I_n(\\theta)=n\\,I_1(\\theta)$.",
  "sec.fisher.p2": "Nonnegativity: $I_1(\\theta)\\ge 0$; equality only in degenerate cases.",
  "sec.fisher.p3": "Reparameterization: for one-to-one $\\phi=g(\\theta)$, $I_1^{(\\phi)}(\\phi)=I_1(\\theta)\\left(\\tfrac{d\\theta}{d\\phi}\\right)^2$.",
  "sec.fisher.ex.title": "Quick sanity check by simulation (Normal mean)",
  "sec.fisher.ex.text": "For $X\\sim\\mathcal N(\\mu,\\sigma^2)$ with known $\\sigma^2$, $I_1(\\mu)=1/\\sigma^2$. The variance of the score $\\partial_\\mu\\log f(X\\mid\\mu)= (X-\\mu)/\\sigma^2$ equals $1/\\sigma^2$.",
  "sec.fisher.ex.code": "set.seed(4)\nsigma <- 2; mu <- 0\nsc <- replicate(20000, { x <- rnorm(1, mu, sigma); (x-mu)/sigma^2 })\ncat(\"empirical Var(score)=\", var(sc), \" target=\", 1/sigma^2, \"\\n\")",
  "sec.fisher.preview": "Next, we will connect Fisher information to optimal estimating equations and asymptotic efficiency. The concepts here provide the variance targets that later procedures aim to achieve."
}
